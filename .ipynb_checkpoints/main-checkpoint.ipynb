{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ENV PARAMS\n",
    "'''\n",
    "train_new_model = False\n",
    "continue_training = None\n",
    "\n",
    "if not train_new_model:\n",
    "    continue_training = False\n",
    "\n",
    "\n",
    "#PATH LOCAL\n",
    "base_path = None #set path to folder containing this notebook here\n",
    "\n",
    "\n",
    "if not train_new_model:\n",
    "    load_path = f'{base_path}best_model.pth'\n",
    "else:\n",
    "    load_path = None\n",
    "path_to_data = f'{base_path}data/USPTO_50K/'\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cpu\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "IMPORTS\n",
    "'''\n",
    "\n",
    "import sys\n",
    "from models import LocalRetro\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import dgl\n",
    "from dgllife.utils import WeaveAtomFeaturizer, \\\n",
    "            CanonicalBondFeaturizer, smiles_to_bigraph, EarlyStopping\n",
    "from dgl.data.utils import save_graphs, load_graphs, Subset\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(f'Training on: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HYPERPARAMETER\n",
    "'''\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "lr = 1e-4\n",
    "#lr scheduler\n",
    "lr_step_size = 10\n",
    "#early stopping\n",
    "patience = 5\n",
    "weight_decay = 1e-6\n",
    "num_epochs = 1 # defual 50\n",
    "print_every = 50\n",
    "max_grad_clip = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PREPROCESSING\n",
    "'''\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import rdkit\n",
    "from rdkit import Chem, RDLogger \n",
    "from rdkit.Chem import rdChemReactions\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "from LocalTemplate.template_extractor import extract_from_reaction\n",
    "from extract_utils import get_reaction_template, reduce_template\n",
    "\n",
    "'''\n",
    "PREPROCESSING OF TRAINING DATA\n",
    "\n",
    "creates reaction templates based on training data\n",
    "\n",
    "IN:  \n",
    "    raw_train.csv            train dataset\n",
    "\n",
    "NOTE: \n",
    "    reaction class is being used!\n",
    "    seperate class numbers are used for atom and bond templates\n",
    "        \n",
    "\n",
    "OUT: \n",
    "    template_rxnclass.csv    templates per reaction USPTO-class\n",
    "    smiles2smarts.csv        contains smarts reaction templates\n",
    "    atom_templates.csv       contains templates for atom change\n",
    "    bond_templates.csv       contains templates for bond change\n",
    "\n",
    "'''\n",
    "if not os.path.exists(f'{base_path}data/USPTO_50K/atom_templates.csv'):\n",
    "    \n",
    "    from extract_utils import remove_reagents, dearomatic, fix_arom, destereo, \\\n",
    "                    clean_smarts, demap\n",
    "\n",
    "    #extract templates\n",
    "    print('extracting templates')\n",
    "    rxns = pd.read_csv(f'{path_to_data}raw_train.csv')['reactants>reagents>production']\n",
    "    class_train = (f'{path_to_data}class_train.csv')\n",
    "    if os.path.exists(class_train):\n",
    "        RXNHASCLASS = True\n",
    "        rxn_class = pd.read_csv(class_train)['class']\n",
    "        template_rxnclass = {i+1:set() for i in range(10)}\n",
    "    else:\n",
    "        RXNHASCLASS = False\n",
    "\n",
    "    smiles2smarts = {}\n",
    "    smiles2edit = {}\n",
    "    smiles2Hs = {}\n",
    "    atom_templates = defaultdict(int)\n",
    "    bond_templates = defaultdict(int)\n",
    "    unique_templates = set()\n",
    "    \n",
    "    for i, rxn in enumerate(rxns):\n",
    "        if RXNHASCLASS:\n",
    "            template_class = rxn_class[i]\n",
    "        try:\n",
    "            rxn, result = get_reaction_template(rxn, i)\n",
    "            if 'reaction_smarts' not in result.keys():\n",
    "                continue\n",
    "            local_template = result['reaction_smarts']\n",
    "            smi_template, sma_template = reduce_template(local_template)\n",
    "            if smi_template not in smiles2smarts.keys(): # first come first serve\n",
    "                template = sma_template\n",
    "                smiles2smarts[smi_template] = sma_template\n",
    "                smiles2edit[smi_template] = result['edit_sites'][2] # keep the map of changing idx\n",
    "                smiles2Hs[smi_template] = result['H_change']\n",
    "            else:\n",
    "                template = smiles2smarts[smi_template]\n",
    "                \n",
    "            edit_sites = result['edit_sites'][0]\n",
    "            \n",
    "            atom_edit = False\n",
    "            bond_edit = False\n",
    "            \n",
    "            for e in edit_sites:\n",
    "                if type(e) == type(1):\n",
    "                    atom_edit = True\n",
    "                else:\n",
    "                    bond_edit = True\n",
    "            \n",
    "            if atom_edit:\n",
    "                atom_templates[template] += 1\n",
    "            if bond_edit:\n",
    "                bond_templates[template] += 1\n",
    "            \n",
    "            unique_templates.add(template)\n",
    "            if RXNHASCLASS:\n",
    "                template_rxnclass[template_class].add(template)\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print('Interrupted')\n",
    "            try:\n",
    "                sys.exit(0)\n",
    "            except SystemExit:\n",
    "                os._exit(0)\n",
    "        except ValueError as e:\n",
    "            print (i, e)\n",
    "        if i % 100 == 0:\n",
    "            print ('\\r i = %s, # of atom template: %s, # of bond template: %s' \\\n",
    "                   % (i, len(atom_templates), len(bond_templates)), end='', flush=True)\n",
    "    print (f'\\n total # of template: {len(unique_templates)}')\n",
    "    derived_templates = {'atom':atom_templates, 'bond': bond_templates}\n",
    "    \n",
    "    if RXNHASCLASS:\n",
    "        pd.DataFrame.from_dict(template_rxnclass, orient = 'index').T.to_csv(\n",
    "            f'{path_to_data}template_rxnclass.csv', index = None)\n",
    "        print(f'written {path_to_data}template_rxnclass.csv')\n",
    "    smiles2smarts = pd.DataFrame({'Smiles_template': k, 'Smarts_template': t,\n",
    "                                  'edit_site':smiles2edit[k], 'change_H': \\\n",
    "                                  smiles2Hs[k]} for k, t in smiles2smarts.items())\n",
    "    smiles2smarts.to_csv(f'{path_to_data}smiles2smarts.csv')\n",
    "    print(f'written {path_to_data}smiles2smarts.csv')\n",
    "    #export template\n",
    "    print(f'exporting templates')\n",
    "    for k in derived_templates.keys():\n",
    "        local_templates = derived_templates[k]\n",
    "        templates = []\n",
    "        template_class = []\n",
    "        template_freq = []\n",
    "        sorted_tuples = sorted(local_templates.items(), key=lambda item: item[1])\n",
    "        c = 1\n",
    "        for t in sorted_tuples:\n",
    "            templates.append(t[0])\n",
    "            template_freq.append(t[1])\n",
    "            template_class.append(c)\n",
    "            c += 1\n",
    "        template_dict = {templates[i]:i+1  for i in range(len(templates)) }\n",
    "        template_df = pd.DataFrame({'Template' : templates, 'Frequency' : template_freq,\n",
    "                                    'Class': template_class})\n",
    "\n",
    "        template_df.to_csv(f'{path_to_data}{k}_templates.csv')\n",
    "        print(f'written {path_to_data}{k}_templates.csv')\n",
    "\n",
    "'''\n",
    "CREATING LABELS FOR WHOLE DATASET\n",
    "\n",
    "IN:  \n",
    "    smiles2smarts.csv        contains smarts reaction templates\n",
    "    atom_templates.csv       contains templates for atom change\n",
    "    bond_templates.csv       contains templates for bond change\n",
    "\n",
    "\n",
    "OUT: \n",
    "    preprocessed_train.csv   contains reaction, products, atom label and bond label\n",
    "    preprocessed_valid.csv   contains reaction, products, atom label and bond label\n",
    "    preprocessed_test.csv   contains reaction, products, atom label and bond label\n",
    "    labeled_data.csv         combines all data of the 3 splits\n",
    "'''    \n",
    "if not os.path.exists(f'{base_path}data/USPTO_50K/labeled_data.csv'):\n",
    "    \n",
    "    #IMPORTS\n",
    "    from preprocessing_utils import matchwithtemp, match_num, get_idx_map, get_edit_site\n",
    "    num_max_edits = 8\n",
    "    threshold = 1\n",
    "    \n",
    "    # load_template_dict\n",
    "    template_dicts = {}\n",
    "    for site in ['atom', 'bond']:\n",
    "        template_df = pd.read_csv(f'{path_to_data}{site}_templates.csv')\n",
    "        template_dict = {template_df['Template'][i]:template_df['Class'][i] \n",
    "                         for i in template_df.index  if template_df['Frequency'][i] >= threshold}\n",
    "        print (f'loaded {len(template_dict)} {site} templates')\n",
    "        template_dicts[site] = template_dict\n",
    "\n",
    "    # load_smi2sma_dict\n",
    "    template_df = pd.read_csv(f'{path_to_data}smiles2smarts.csv')\n",
    "    smiles2smarts = {template_df['Smiles_template'][i]:template_df['Smarts_template'][i] \\\n",
    "                     for i in template_df.index}\n",
    "    smiles2edit = {template_df['Smiles_template'][i]:template_df['edit_site'][i] \\\n",
    "                   for i in template_df.index}\n",
    "\n",
    "    \n",
    "    \n",
    "    pre_sets = []\n",
    "    split_names = ['train', 'val', 'test']\n",
    "    # labeling_dataset   (args, split,\n",
    "    for split in split_names:\n",
    "        atom_templates = template_dicts['atom']\n",
    "        bond_templates = template_dicts['bond']\n",
    "        rxns = pd.read_csv(f'{path_to_data}raw_{split}.csv')['reactants>reagents>production']\n",
    "        products = []\n",
    "        atom_labels = []\n",
    "        bond_labels = []\n",
    "        masks = []\n",
    "        success = 0\n",
    "        for n, rxn in enumerate(rxns):\n",
    "            product = rxn.split('>>')[1]\n",
    "            try:\n",
    "                rxn, result = get_reaction_template(rxn, n)\n",
    "                local_template = result['reaction_smarts']\n",
    "                smi_template, sma_template = reduce_template(local_template)\n",
    "                if smi_template not in smiles2smarts.keys():\n",
    "                    products.append(product)\n",
    "                    atom_labels.append(0)\n",
    "                    bond_labels.append(0)\n",
    "                    masks.append(0)\n",
    "                    continue\n",
    "                else:\n",
    "                    replace_dict = matchwithtemp(sma_template, smiles2smarts[smi_template], \n",
    "                                                 result['replacement_dict']) \n",
    "                    replace_dict = get_idx_map(product, replace_dict)\n",
    "                    edit_sites = eval(match_num(smiles2edit[smi_template], replace_dict))\n",
    "                    local_template = smiles2smarts[smi_template]\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print('Interrupted')\n",
    "                try:\n",
    "                    sys.exit(0)\n",
    "                except SystemExit:\n",
    "                    os._exit(0)\n",
    "            except Exception as e:\n",
    "                products.append(product)\n",
    "                atom_labels.append(0)\n",
    "                bond_labels.append(0)\n",
    "                masks.append(0)\n",
    "                continue\n",
    "\n",
    "            if len(edit_sites) <= num_max_edits:\n",
    "                atom_sites, bond_sites = get_edit_site(product)\n",
    "                try:\n",
    "                    if local_template not in atom_templates.keys() and \\\n",
    "                            local_template not in bond_templates.keys():\n",
    "                        products.append(product)\n",
    "                        atom_labels.append(0)\n",
    "                        bond_labels.append(0)\n",
    "                        masks.append(0)\n",
    "                    else:\n",
    "                        atom_label = [0] * len(atom_sites)\n",
    "                        bond_label = [0] * len(bond_sites)\n",
    "                        for edit_site in edit_sites:\n",
    "                            if type(edit_site) == type(1):\n",
    "                                atom_label[atom_sites.index(edit_site)] = atom_templates[local_template]\n",
    "                            else:\n",
    "                                bond_label[bond_sites.index(edit_site)] = bond_templates[local_template]\n",
    "                        products.append(product)\n",
    "                        atom_labels.append(atom_label)\n",
    "                        bond_labels.append(bond_label)\n",
    "                        masks.append(1)\n",
    "                        success += 1\n",
    "                except Exception as e:\n",
    "                    products.append(product)\n",
    "                    atom_labels.append(0)\n",
    "                    bond_labels.append(0)\n",
    "                    masks.append(0)\n",
    "                    continue\n",
    "\n",
    "                if n % 100 == 0:\n",
    "                    print ('\\r Processing USPTO_50K %s data..., success %s data (%s/%s)' \\\n",
    "                           % (split, success, n, len(rxns)), end='', flush=True)\n",
    "            else:\n",
    "                print ('\\nReaction # %s has too many (%s) edits... may be wrong mapping!'\n",
    "                       % (n, len(edit_sites)))\n",
    "                products.append(product)\n",
    "                atom_labels.append(0)\n",
    "                bond_labels.append(0)\n",
    "    # begin edit\n",
    "                masks.append(0)\n",
    "    # end edit\n",
    "            \n",
    "        print ('\\nDerived tempalates cover %.3f of %s data reactions' % ((success/len(rxns)), split))\n",
    "    # begin cheap trick\n",
    "        if len(masks) != len(rxns):\n",
    "            diff = len(rxns) - len(masks)\n",
    "            count_diff = 0\n",
    "            for _ in range(diff):\n",
    "                count_diff += 1\n",
    "                masks.append(0)\n",
    "            print(f'cheap trick applied {count_diff} times')\n",
    "    # end cheap trick\n",
    "        df = pd.DataFrame({'Reaction': rxns, 'Products': products, 'Atom_label': atom_labels, \\\n",
    "                           'Bond_label': bond_labels, 'Mask': masks})\n",
    "        df.to_csv(f'{base_path}data/USPTO_50K/preprocessed_{split}.csv')\n",
    "        print(f'written {base_path}data/USPTO_50K/preprocessed_{split}.csv')\n",
    "        pre_sets.append(df)\n",
    "\n",
    "\n",
    "    # combine_preprocessed_data\n",
    "    train_valid = pre_sets[0][pre_sets[0]['Mask'] != 0].reset_index()\n",
    "    val_valid = pre_sets[1][pre_sets[1]['Mask'] != 0].reset_index()\n",
    "    test_valid = pre_sets[2][pre_sets[2]['Mask'] != 0].reset_index()\n",
    "    \n",
    "    train_valid['Split'] = ['train'] * len(train_valid)\n",
    "    val_valid['Split'] = ['val'] * len(val_valid)\n",
    "    test_valid['Split'] = ['test'] * len(test_valid)\n",
    "\n",
    "    all_valid = train_valid.append(val_valid, ignore_index=True)\n",
    "    all_valid = all_valid.append(test_valid, ignore_index=True)\n",
    "    print (f'Valid data size: {len(all_valid)}')\n",
    "    all_valid.to_csv(f'{path_to_data}labeled_data.csv', index = None)\n",
    "    print(f'written {path_to_data}labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously saved dgl graphs...\n",
      "Dataset loaded with len 49944, creating subsets...\n",
      "created loaders: train 40000 val 4976 test 4992\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DATASET\n",
    "\n",
    "loading graphs or create them if they don't exist\n",
    "\n",
    "IN:  \n",
    "    (labeled_data.csv)         combines all data of the 3 splits\n",
    "\n",
    "OUT: \n",
    "    USPTO_50K_dglgraph.bin     contains graphs of labeled_data.csv\n",
    "    train_loader\n",
    "    val_loader\n",
    "    test_loader\n",
    "'''    \n",
    "def flatten_list(t):\n",
    "    return torch.LongTensor([item for sublist in t for item in sublist])\n",
    "\n",
    "def collate_molgraphs(data):\n",
    "    smiles, graphs, atom_labels, bond_labels = map(list, zip(*data))\n",
    "    atom_labels = flatten_list(atom_labels)\n",
    "    bond_labels = flatten_list(bond_labels)\n",
    "    bg = dgl.batch(graphs)\n",
    "    bg.set_n_initializer(dgl.init.zero_initializer)\n",
    "    bg.set_e_initializer(dgl.init.zero_initializer)\n",
    "    return smiles, bg, atom_labels, bond_labels\n",
    "\n",
    "class USPTODataset(object):\n",
    "    def __init__(self, base_path, load=True, log_every=1000):\n",
    "        df = pd.read_csv(f'{path_to_data}labeled_data.csv')\n",
    "        self.train_ids = df.index[df['Split'] == 'train'].values\n",
    "        self.val_ids = df.index[df['Split'] == 'val'].values\n",
    "        self.test_ids = df.index[df['Split'] == 'test'].values\n",
    "        self.smiles = df['Products'].tolist()\n",
    "        self.atom_labels = [eval(t) for t in df['Atom_label']]\n",
    "        self.bond_labels = [eval(t) for t in df['Bond_label']]\n",
    "        self.cache_file_path = f'{base_path}/data/saved_graphs/USPTO_50K_dglgraph.bin'\n",
    "        self._pre_process(load, log_every)\n",
    "\n",
    "    def _pre_process(self, load, log_every):\n",
    "        if os.path.exists(self.cache_file_path) and load:\n",
    "            print('Loading previously saved dgl graphs...')\n",
    "            self.graphs, label_dict = load_graphs(self.cache_file_path)\n",
    "        else:\n",
    "            print('Processing dgl graphs from scratch...')\n",
    "            self.graphs = []\n",
    "            for i, s in enumerate(self.smiles):\n",
    "                if (i + 1) % log_every == 0:\n",
    "                    print('\\rProcessing molecule %d/%d' % (i+1, \n",
    "                                                    len(self.smiles)), end='', flush=True)\n",
    "                self.graphs.append(smiles_to_bigraph(s, add_self_loop=True, \n",
    "                            node_featurizer=WeaveAtomFeaturizer(), \n",
    "                            edge_featurizer=CanonicalBondFeaturizer(self_loop=True), \n",
    "                                                     canonical_atom_order=False))\n",
    "            print ()\n",
    "            save_graphs(self.cache_file_path, self.graphs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.smiles[item], self.graphs[item], self.atom_labels[item], self.bond_labels[item]\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.smiles)\n",
    "        \n",
    "        \n",
    "dataset = USPTODataset(base_path)\n",
    "print(f'Dataset loaded with len {len(dataset)}, creating subsets...')\n",
    "train_set = Subset(dataset, dataset.train_ids)\n",
    "val_set   = Subset(dataset, dataset.val_ids)\n",
    "test_set  = Subset(dataset, dataset.test_ids)\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=collate_molgraphs, num_workers=num_workers)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=collate_molgraphs, num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=collate_molgraphs, num_workers=num_workers)\n",
    "print(f'''created loaders: train {len(train_loader)*batch_size} \\\n",
    "val {len(val_loader)*batch_size} test {len(test_loader)*batch_size}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model data, ready for inference\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CREATE/LOAD MODEL\n",
    "\n",
    "IN:\n",
    "    (load_path)\n",
    "\n",
    "OUT: \n",
    "    model\n",
    "'''    \n",
    "def get_configure():\n",
    "    config = {\n",
    "              \"attention_heads\": 8,\n",
    "              \"attention_layers\": 1,\n",
    "              \"edge_hidden_feats\": 64,\n",
    "              \"node_out_feats\": 320,\n",
    "              \"num_step_message_passing\": 6\n",
    "            }\n",
    "    config['AtomTemplate_n'] = len(pd.read_csv(f'{path_to_data}atom_templates.csv'))\n",
    "    config['BondTemplate_n'] = len(pd.read_csv(f'{path_to_data}bond_templates.csv'))\n",
    "    config['in_node_feats'] = WeaveAtomFeaturizer().feat_size()\n",
    "    config['in_edge_feats'] = CanonicalBondFeaturizer(self_loop=True).feat_size()\n",
    "    config['GRA'] = True\n",
    "    return config\n",
    "\n",
    "def get_model():\n",
    "    exp_config = get_configure()\n",
    "    model = LocalRetro(\n",
    "        node_in_feats=exp_config['in_node_feats'],\n",
    "        edge_in_feats=exp_config['in_edge_feats'],\n",
    "        node_out_feats=exp_config['node_out_feats'],\n",
    "        edge_hidden_feats=exp_config['edge_hidden_feats'],\n",
    "        num_step_message_passing=exp_config['num_step_message_passing'],\n",
    "        attention_heads = exp_config['attention_heads'],\n",
    "        attention_layers = exp_config['attention_layers'],\n",
    "        AtomTemplate_n = exp_config['AtomTemplate_n'],\n",
    "        BondTemplate_n = exp_config['BondTemplate_n'],\n",
    "        GRA = exp_config['GRA'])\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_model(load_path, base_path):\n",
    "\n",
    "    return model, optimizer, epoch, loss\n",
    "\n",
    "if train_new_model == False:\n",
    "    assert os.path.exists(load_path)\n",
    "    model = get_model()\n",
    "    if torch.cuda.is_available():\n",
    "        checkpoint = torch.load(load_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(load_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if continue_training:\n",
    "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']     \n",
    "        print(f'loaded model data, ready for training')\n",
    "        \n",
    "    else:\n",
    "        print(f'loaded model data, ready for inference')\n",
    "\n",
    "\n",
    "elif os.path.exists(f'{base_path}models/model_{date.today()}.pth'):\n",
    "    print(f'this path already exists: {base_path}models/model_{date.today()}.pth')\n",
    "    \n",
    "else:\n",
    "    print(f'no model found, created this new one:')\n",
    "    model = get_model()\n",
    "    print(model)\n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared for inference: cleared test dataset variables\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TRAIN MODEL\n",
    "\n",
    "IN:\n",
    "    \n",
    "\n",
    "OUT: \n",
    "    model from early stopping\n",
    "    additionally: model_state_dict to continue training\n",
    "'''    \n",
    "\n",
    "if continue_training or train_new_model:\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_step_size)\n",
    "    stopper = EarlyStopping(mode='lower', patience=patience)\n",
    "    for epoch in range(num_epochs):\n",
    "        ### TRAIN MODEL\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for batch_id, batch_data in enumerate(tqdm(train_loader)):\n",
    "            smiles, bg, atom_labels, bond_labels = batch_data\n",
    "            if len(smiles) == 1:\n",
    "                continue\n",
    "            atom_labels, bond_labels = atom_labels.to(device), bond_labels.to(device)\n",
    "\n",
    "            #predict\n",
    "            bg = bg.to(device)\n",
    "            node_feats = bg.ndata.pop('h').to(device)\n",
    "            edge_feats = bg.edata.pop('e').to(device)\n",
    "            atom_logits, bond_logits, _ = model(bg, node_feats, edge_feats)\n",
    "\n",
    "            loss_a = loss_criterion(atom_logits, atom_labels).mean()\n",
    "            loss_b = loss_criterion(bond_logits, bond_labels).mean()\n",
    "            total_loss = loss_a + loss_b\n",
    "\n",
    "            train_loss += total_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()      \n",
    "            total_loss.backward() \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        print('\\nepoch %d/%d, training loss: %.4f' % (epoch + 1, num_epochs, train_loss/batch_id))\n",
    "\n",
    "        ### VALIDATE MODEL    \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_id, batch_data in enumerate(tqdm(val_loader)):\n",
    "                smiles, bg, atom_labels, bond_labels = batch_data\n",
    "                atom_labels, bond_labels = atom_labels.to(device), bond_labels.to(device)\n",
    "\n",
    "                bg = bg.to(device)\n",
    "                node_feats = bg.ndata.pop('h').to(device)\n",
    "                edge_feats = bg.edata.pop('e').to(device)\n",
    "                atom_logits, bond_logits, _ = model(bg, node_feats, edge_feats)\n",
    "\n",
    "                loss_a = loss_criterion(atom_logits, atom_labels).mean()\n",
    "                loss_b = loss_criterion(bond_logits, bond_labels).mean()\n",
    "                total_loss = loss_a + loss_b\n",
    "                val_loss += total_loss.item()\n",
    "\n",
    "        val_loss = val_loss/batch_id            \n",
    "        early_stop = stopper.step(val_loss, model) #maybe detach loss here?\n",
    "        scheduler.step()\n",
    "        print('epoch %d/%d, validation loss: %.4f' %  (epoch + 1, num_epochs, val_loss))\n",
    "        print('epoch %d/%d, Best val loss: %.4f' % (epoch + 1, num_epochs, stopper.best_score))\n",
    "        if early_stop:\n",
    "            print ('Early stopped!!')\n",
    "            break\n",
    "\n",
    "    stopper.load_checkpoint(model)\n",
    "\n",
    "    ### TEST MODEL\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(tqdm(test_loader)):\n",
    "            smiles, bg, atom_labels, bond_labels = batch_data\n",
    "            atom_labels, bond_labels = atom_labels.to(device), bond_labels.to(device)\n",
    "\n",
    "            bg = bg.to(device)\n",
    "            node_feats = bg.ndata.pop('h').to(device)\n",
    "            edge_feats = bg.edata.pop('e').to(device)\n",
    "            atom_logits, bond_logits, _ = model(bg, node_feats, edge_feats)\n",
    "\n",
    "            loss_a = loss_criterion(atom_logits, atom_labels).mean()\n",
    "            loss_b = loss_criterion(bond_logits, bond_labels).mean()\n",
    "            total_loss = loss_a + loss_b\n",
    "            test_loss += total_loss.item()\n",
    "\n",
    "    test_loss = test_loss/batch_id \n",
    "    print('test loss: %.4f' % test_loss)\n",
    "\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss' : train_loss\n",
    "                }, f'{base_path}models/model_train_{date.today()}.pth')\n",
    "    print(f'best model additionally saved at: {base_path}models/model_train_{date.today()}.pth')\n",
    "\n",
    "test_loader = None\n",
    "test_set = None\n",
    "print(f'prepared for inference: cleared test dataset variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously saved test dgl graphs...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "PREPROCESSING TEST DATA\n",
    "\n",
    "create graphs from raw test data\n",
    "\n",
    "IN:\n",
    "    raw_test.csv\n",
    "\n",
    "OUT: \n",
    "    USPTO_50K_test_dglgraph.bin\n",
    "'''    \n",
    "canonicalize_data = True\n",
    "use_reduced_testset = False\n",
    "overwrite_inf = True\n",
    "\n",
    "\n",
    "if use_reduced_testset:\n",
    "    result_path = f'{base_path}outputs/reduced_testset/'\n",
    "else:\n",
    "    result_path = f'{base_path}outputs/'\n",
    "\n",
    "\n",
    "\n",
    "def collate_molgraphs_test(data):\n",
    "    smiles, graphs, rxns = map(list, zip(*data))\n",
    "    bg = dgl.batch(graphs)\n",
    "    bg.set_n_initializer(dgl.init.zero_initializer)\n",
    "    bg.set_e_initializer(dgl.init.zero_initializer)\n",
    "    return smiles, bg, rxns\n",
    "\n",
    "def canonicalize_smi(smi, is_smarts=False, remove_atom_mapping=True):\n",
    "    \"\"\"\n",
    "    Canonicalize SMARTS from https://github.com/rxn4chemistry/rxnfp/blob/master/rxnfp/tokenization.py#L249\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if not mol:\n",
    "        raise ValueError(\"Molecule not canonicalizable\")\n",
    "    if remove_atom_mapping:\n",
    "        for atom in mol.GetAtoms():\n",
    "            if atom.HasProp(\"molAtomMapNumber\"):\n",
    "                atom.ClearProp(\"molAtomMapNumber\")\n",
    "    return Chem.MolToSmiles(mol)\n",
    "\n",
    "\n",
    "class USPTOTestDataset(object):\n",
    "    def __init__(self, canonicalize_data, use_reduced_testset, load=True, log_every=1000):\n",
    "        self.canonicalize = canonicalize_data\n",
    "        if use_reduced_testset:\n",
    "            df = pd.read_csv(f'{path_to_data}raw_test_reduced.csv')\n",
    "        else:\n",
    "            df = pd.read_csv(f'{path_to_data}raw_test.csv')\n",
    "        self.rxns = df['reactants>reagents>production'].tolist()\n",
    "        self.smiles = [rxn.split('>>')[-1] for rxn in self.rxns]\n",
    "        if self.canonicalize and not use_reduced_testset:\n",
    "            for _ in range(2):\n",
    "                self.smiles = [canonicalize_smi(smi) for smi in self.smiles]\n",
    "            self.cache_file_path = f'{base_path}data/saved_graphs/USPTO_50K_test_dglgraph_can.bin'\n",
    "        elif not self.canonicalize and not use_reduced_testset:\n",
    "            self.cache_file_path = f'{base_path}data/saved_graphs/USPTO_50K_test_dglgraph.bin'\n",
    "        elif self.canonicalize and use_reduced_testset:\n",
    "            for _ in range(2):\n",
    "                self.smiles = [canonicalize_smi(smi) for smi in self.smiles]\n",
    "            self.cache_file_path = f'{base_path}data/saved_graphs/USPTO_50K_test_reduced_dglgraph_can.bin'\n",
    "        elif not self.canonicalize and use_reduced_testset:\n",
    "            self.cache_file_path = f'{base_path}data/saved_graphs/USPTO_50K_test_reduced_dglgraph.bin' \n",
    "        self._pre_process(load, log_every)\n",
    "        \n",
    "\n",
    "    def _pre_process(self,load, log_every):\n",
    "        if os.path.exists(self.cache_file_path) and load:\n",
    "            print('Loading previously saved test dgl graphs...')\n",
    "            self.graphs, label_dict = load_graphs(self.cache_file_path)\n",
    "        else:\n",
    "            print('Processing test dgl graphs from scratch...')\n",
    "            self.graphs = []\n",
    "            for i, s in enumerate(self.smiles):\n",
    "                if (i + 1) % log_every == 0:\n",
    "                    print('Processing molecule %d/%d' % (i+1, len(self.smiles)))\n",
    "                self.graphs.append(smiles_to_bigraph(s, add_self_loop=True, \n",
    "                                                   node_featurizer=WeaveAtomFeaturizer(),\n",
    "                                                   edge_featurizer=CanonicalBondFeaturizer(self_loop=True), \n",
    "                                                   canonical_atom_order=False))\n",
    "            save_graphs(self.cache_file_path, self.graphs)\n",
    "            print(f'saved graphs to: {self.cache_file_path}')\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "            return self.smiles[item], self.graphs[item], self.rxns[item]\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.smiles)\n",
    "\n",
    "\n",
    "\n",
    "test_set = USPTOTestDataset(canonicalize_data, use_reduced_testset)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size,\n",
    "                         collate_fn=collate_molgraphs_test, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test molecule batch 313/313\n",
      " written results to: /home/dominik/AI_Master/Project/my_solution/outputs/raw_predictions_can.txt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "INFERENCE\n",
    "\n",
    "\n",
    "IN:\n",
    "    test_loader\n",
    "\n",
    "OUT: \n",
    "    raw_predictions.txt\n",
    "\n",
    "''' \n",
    "#params for inference\n",
    "top_num = 100 #default 100\n",
    "\n",
    "\n",
    "configs = get_configure()\n",
    "\n",
    "if use_reduced_testset and canonicalize_data:\n",
    "    raw_path = f'{result_path}reduced_testset_raw_predictions_can.txt'\n",
    "elif use_reduced_testset and not canonicalize_data:\n",
    "    raw_path = f'{result_path}reduced_testset_raw_predictions.txt'\n",
    "elif not use_reduced_testset and canonicalize_data:\n",
    "    raw_path = f'{result_path}raw_predictions_can.txt'\n",
    "elif not use_reduced_testset and not canonicalize_data:\n",
    "    raw_path = f'{result_path}raw_predictions.txt'\n",
    "\n",
    "def get_id_template(a, CLASS_NUM):\n",
    "    CLASS_NUM = CLASS_NUM + 1 # no template\n",
    "    edit_idx = a//CLASS_NUM\n",
    "    template = a%CLASS_NUM\n",
    "    return (edit_idx, template)\n",
    "\n",
    "def output2edit(out, CLASS_NUM, top_num):\n",
    "    readout = out.cpu().detach().numpy()\n",
    "    readout = readout.reshape(-1)\n",
    "    output_rank = np.flip(np.argsort(readout))\n",
    "    output_rank = [r for r in output_rank if get_id_template(r, CLASS_NUM)[1] != 0][:top_num]\n",
    "    \n",
    "    selected_edit = [get_id_template(a, CLASS_NUM) for a in output_rank]\n",
    "    selected_proba = [readout[a] for a in output_rank]\n",
    "     \n",
    "    return selected_edit, selected_proba\n",
    "    \n",
    "def combined_edit(graph, atom_out, bond_out, ATOM_CLASS, BOND_CLASS, top_num):\n",
    "    edit_id_a, edit_proba_a = output2edit(atom_out, ATOM_CLASS, top_num)\n",
    "    edit_id_b, edit_proba_b = output2edit(bond_out, BOND_CLASS, top_num)\n",
    "    atom_pair_list = torch.transpose(graph.adjacency_matrix().coalesce().indices(), 0, 1).numpy()\n",
    "    edit_id_b = [(list(atom_pair_list[edit_id[0]]), edit_id[1])  for edit_id in edit_id_b]\n",
    "    edit_id_c = edit_id_a + edit_id_b\n",
    "    edit_proba_c = edit_proba_a + edit_proba_b\n",
    "    edit_rank_c = np.flip(np.argsort(edit_proba_c))[:top_num]\n",
    "    edit_id_c = [edit_id_c[r] for r in edit_rank_c]\n",
    "    edit_proba_c = [edit_proba_c[r] for r in edit_rank_c]\n",
    "    \n",
    "    return edit_id_c, edit_proba_c\n",
    "\n",
    "if overwrite_inf:\n",
    "    model.eval()\n",
    "    with open(raw_path, 'w') as f:\n",
    "        f.write('Test_id\\tReaction\\t%s\\n' % '\\t'.join(['Edit %s\\tProba %s' % (i+1, i+1) \\\n",
    "                                                       for i in range(top_num)]))\n",
    "        with torch.no_grad():\n",
    "            for batch_id, data in enumerate(test_loader):\n",
    "                _, bg, rxns = data\n",
    "\n",
    "                #predict\n",
    "                bg = bg.to(device)\n",
    "                node_feats = bg.ndata.pop('h').to(device)\n",
    "                edge_feats = bg.edata.pop('e').to(device)\n",
    "                batch_atom_logits, batch_bond_logits, _ = model(bg, node_feats, edge_feats)\n",
    "\n",
    "                batch_atom_logits = nn.Softmax(dim=1)(batch_atom_logits)\n",
    "                batch_bond_logits = nn.Softmax(dim=1)(batch_bond_logits) \n",
    "\n",
    "                sg = bg.remove_self_loop()\n",
    "                graphs = dgl.unbatch(sg, sg.batch_num_nodes(), (sg.batch_num_edges() - sg.batch_num_nodes()))\n",
    "                nodes_sep = [0]\n",
    "                edges_sep = [0]\n",
    "                for g in graphs:\n",
    "                    nodes_sep.append(nodes_sep[-1] + g.num_nodes())\n",
    "                    edges_sep.append(edges_sep[-1] + g.num_edges())\n",
    "                nodes_sep = nodes_sep[1:]\n",
    "                edges_sep = edges_sep[1:]\n",
    "\n",
    "\n",
    "                start_node = 0\n",
    "                start_edge = 0\n",
    "                print('\\rWriting test molecule batch %s/%s' % (batch_id+1, len(test_loader)), end='', flush=True)\n",
    "                for single_id, (graph, end_node, end_edge) in enumerate(zip(graphs, nodes_sep, edges_sep)):\n",
    "                    rxn = rxns[single_id]\n",
    "                    test_id = (batch_id * batch_size) + single_id\n",
    "                    edit_id, edit_proba = combined_edit(graph, batch_atom_logits[start_node:end_node], \n",
    "                                    batch_bond_logits[start_edge:end_edge], configs['AtomTemplate_n'], \n",
    "                                                        configs['BondTemplate_n'], top_num)\n",
    "                    start_node = end_node\n",
    "                    start_edge = end_edge\n",
    "                    f.write('%s\\t%s\\t%s\\n' % (test_id, rxn, '\\t'.join(['%s\\t%.3f' % \\\n",
    "                                            (edit_id[i], edit_proba[i]) for i in range(top_num)])))\n",
    "\n",
    "    print (f'\\n written results to: {raw_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding LocalRetro predictions 5007/5007\n",
      " num of excteptions when applying templates for decoding: 39\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DECODING PREDICTIONS\n",
    "\n",
    "applying templates to raw predictions\n",
    "\n",
    "IN:\n",
    "    raw_predictions.txt\n",
    "    class_test.csv           raw_test.csv with class labels\n",
    "    smiles2smarts.csv        contains smarts reaction templates\n",
    "    atom_templates.csv       contains templates for atom change\n",
    "    bond_templates.csv       contains templates for bond change\n",
    "\n",
    "OUT: \n",
    "    decoded_predictions.txt  containing predicted reactants\n",
    "    decoded_predictions_class.txt\n",
    "                             decoded_predictions filtered by class value from USPTO database\n",
    "\n",
    "''' \n",
    "top_k = 100 #default 50\n",
    "\n",
    "from LocalTemplate.template_decoder import *\n",
    "\n",
    "\n",
    "\n",
    "atom_templates = pd.read_csv(f'{path_to_data}atom_templates.csv')\n",
    "bond_templates = pd.read_csv(f'{path_to_data}bond_templates.csv')\n",
    "smiles2smarts = pd.read_csv(f'{path_to_data}smiles2smarts.csv')\n",
    "class_test = f'{path_to_data}class_test.csv'\n",
    "if os.path.exists(class_test):\n",
    "    rxn_class_given = True\n",
    "    templates_class = pd.read_csv(f'{path_to_data}template_rxnclass.csv')\n",
    "    test_rxn_class = pd.read_csv(f'{path_to_data}class_test.csv')['class']\n",
    "else:\n",
    "    rxn_class_given = False \n",
    "\n",
    "    \n",
    "if overwrite_inf:\n",
    "    counter = 0\n",
    "    atom_templates = {atom_templates['Class'][i]: atom_templates['Template'][i] \n",
    "                                                for i in atom_templates.index}\n",
    "    bond_templates = {bond_templates['Class'][i]: bond_templates['Template'][i] \n",
    "                                                for i in bond_templates.index}\n",
    "    smarts2E = {smiles2smarts['Smarts_template'][i]: eval(smiles2smarts['edit_site'][i]) \n",
    "                                                for i in smiles2smarts.index}\n",
    "    smarts2H = {smiles2smarts['Smarts_template'][i]: eval(smiles2smarts['change_H'][i])\n",
    "                                                for i in smiles2smarts.index}\n",
    "\n",
    "    prediction = pd.read_csv(raw_path, sep = '\\t')\n",
    "\n",
    "    if use_reduced_testset and canonicalize_data:\n",
    "        output_path = f'{result_path}reduced_testset_decoded_prediction_can.txt'\n",
    "        output_path_class = f'{result_path}reduced_testset_decoded_prediction_class_can.txt'\n",
    "    elif use_reduced_testset and not canonicalize_data:\n",
    "        output_path = f'{result_path}reduced_testset_decoded_prediction.txt'\n",
    "        output_path_class = f'{result_path}reduced_testset_decoded_prediction_class.txt'\n",
    "    elif not use_reduced_testset and canonicalize_data:\n",
    "        output_path = f'{result_path}decoded_prediction_can.txt'\n",
    "        output_path_class = f'{result_path}decoded_prediction_class_can.txt'\n",
    "    elif not use_reduced_testset and not canonicalize_data:\n",
    "        output_path = f'{result_path}decoded_prediction.txt'\n",
    "        output_path_class = f'{result_path}decoded_prediction_class.txt'    \n",
    "\n",
    "\n",
    "    with open(output_path, 'w') as f1, open(output_path_class, 'w') as f2:\n",
    "        for i in prediction.index:\n",
    "            all_prediction = []\n",
    "            class_prediction = []\n",
    "            rxn = prediction['Reaction'][i]\n",
    "            products = rxn.split('>>')[1]\n",
    "            idx_map = get_idx_map(products)\n",
    "            for K_prediciton in prediction.columns:\n",
    "                if 'Edit' not in K_prediciton:\n",
    "                    continue\n",
    "                edition = eval(prediction[K_prediciton][i])\n",
    "                edit_idx = edition[0]\n",
    "                template_class = edition[1]\n",
    "                if type(edit_idx) == type(0):\n",
    "                    template = atom_templates[template_class]\n",
    "                    if len(template.split('>>')[0].split('.')) > 1:\n",
    "                        edit_idx = idx_map[edit_idx]\n",
    "                else:\n",
    "                    template = bond_templates[template_class]\n",
    "                    edit_idx = tuple(edit_idx)\n",
    "                    if len(template.split('>>')[0].split('.')) > 1:\n",
    "                        edit_idx = (idx_map[edit_idx[0]], idx_map[edit_idx[1]])\n",
    "\n",
    "                template_idx = smarts2E[template]\n",
    "                H_change = smarts2H[template]\n",
    "                try:\n",
    "                    pred_reactants, _, _ = apply_template(products, template, edit_idx,\n",
    "                                                          template_idx, H_change)\n",
    "                except Exception as e:\n",
    "                    # print (e)\n",
    "                    counter += 1\n",
    "                    pred_reactants = []\n",
    "\n",
    "                if len(pred_reactants) == 0:\n",
    "                    try:\n",
    "                        template = dearomatic(template)\n",
    "                        pred_reactants, _, _ = apply_template(products, template, edit_idx,\n",
    "                                                              template_idx, H_change)\n",
    "                    except:\n",
    "                        pred_reactants = []\n",
    "\n",
    "                all_prediction += [p for p in pred_reactants if p not in all_prediction]\n",
    "\n",
    "                if rxn_class_given:\n",
    "                    rxn_class = test_rxn_class[i]\n",
    "                    if template in templates_class[str(rxn_class)].values:\n",
    "                        class_prediction += [p for p in pred_reactants if p \n",
    "                                             not in class_prediction]\n",
    "                    if len (class_prediction) >= top_k:\n",
    "                        break\n",
    "\n",
    "                elif len (all_prediction) >= top_k:\n",
    "                    break\n",
    "\n",
    "            f1.write('\\t'.join(all_prediction) + '\\n')\n",
    "            f2.write('\\t'.join(class_prediction) + '\\n')\n",
    "            print('\\rDecoding LocalRetro predictions %d/%d' % \\\n",
    "                  (i+1, len(prediction)), end='', flush=True)\n",
    "    print(f'\\n num of excteptions when applying templates for decoding: {counter}')\n",
    "    with open('log.txt', 'a') as f:\n",
    "        f.write(f'\\nnum of excteptions when applying templates for decoding: {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating accuracy... 5000/50075007\n",
      "Top-1 Exact accuracy: 0.064, MaxFrag accuracy: 0.067\n",
      "Top-3 Exact accuracy: 0.088, MaxFrag accuracy: 0.093\n",
      "Top-5 Exact accuracy: 0.097, MaxFrag accuracy: 0.103\n",
      "Top-10 Exact accuracy: 0.107, MaxFrag accuracy: 0.115\n",
      "Top-50 Exact accuracy: 0.118, MaxFrag accuracy: 0.131\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TOP K\n",
    "\n",
    "calculate top k accuracy\n",
    "\n",
    "\n",
    "IN:\n",
    "    decoded_predictions.txt  containing predicted reactants\n",
    "    decoded_predictions_class.txt\n",
    "                             decoded_predictions filtered by class value from USPTO database\n",
    "\n",
    "OUT: \n",
    "\n",
    "''' \n",
    "\n",
    "\n",
    "from rdkit.Chem.EnumerateStereoisomers import EnumerateStereoisomers\n",
    "\n",
    "\n",
    "def demap(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom.SetAtomMapNum(0)\n",
    "    smi = Chem.MolToSmiles(mol)\n",
    "    return Chem.MolToSmiles(Chem.MolFromSmiles(smi))\n",
    "    \n",
    "def get_isomers(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    isomers = tuple(EnumerateStereoisomers(mol))\n",
    "    isomers_smi = [Chem.MolToSmiles(x, isomericSmiles=True) for x in isomers]\n",
    "    return isomers_smi\n",
    "    \n",
    "def get_MaxFrag(smiles):\n",
    "    return max(smiles.split('.'), key=len)\n",
    "\n",
    "def isomer_match(preds, reac, MaxFrag = False):\n",
    "    try:\n",
    "        if MaxFrag:\n",
    "            reac = get_MaxFrag(reac)\n",
    "        reac_isomers = get_isomers(reac)\n",
    "        for k, pred in enumerate(preds):\n",
    "            if MaxFrag:\n",
    "                pred = get_MaxFrag(pred)\n",
    "            pred_isomers = get_isomers(pred)\n",
    "            if(set(pred_isomers).issubset(set(reac_isomers))):\n",
    "                return k+1\n",
    "        return -1\n",
    "    except:\n",
    "        return -1\n",
    "if use_reduced_testset:\n",
    "    test_file = pd.read_csv(f'{path_to_data}raw_test_reduced.csv')    \n",
    "else:\n",
    "    test_file = pd.read_csv(f'{path_to_data}raw_test.csv')\n",
    "\n",
    "rxn_ps = [rxn.split('>>')[1] for rxn in \n",
    "          test_file['reactants>reagents>production']]\n",
    "ground_truth = [demap(rxn.split('>>')[0]) for rxn in \n",
    "                test_file['reactants>reagents>production']]\n",
    "\n",
    "\n",
    "class_given = False\n",
    "\n",
    "if class_given:\n",
    "    if use_reduced_testset and canonicalize_data:\n",
    "        result_file = f'{result_path}reduced_testset_decoded_prediction_class_can.txt'\n",
    "    elif use_reduced_testset and not canonicalize_data:\n",
    "        result_file = f'{result_path}reduced_testset_decoded_prediction_class.txt'\n",
    "    elif not use_reduced_testset and canonicalize_data:\n",
    "        result_file = f'{result_path}decoded_prediction_class_can.txt'\n",
    "    elif not use_reduced_testset and not canonicalize_data:\n",
    "        result_file = f'{result_path}decoded_prediction_class.txt'  \n",
    "else:\n",
    "    if use_reduced_testset and canonicalize_data:\n",
    "        result_file = f'{result_path}reduced_testset_decoded_prediction_can.txt'\n",
    "    elif use_reduced_testset and not canonicalize_data:\n",
    "        result_file = f'{result_path}reduced_testset_decoded_prediction.txt'\n",
    "    elif not use_reduced_testset and canonicalize_data:\n",
    "        result_file = f'{result_path}decoded_prediction_can.txt'\n",
    "    elif not use_reduced_testset and not canonicalize_data:\n",
    "        result_file = f'{result_path}decoded_prediction.txt'\n",
    "\n",
    "results = {}\n",
    "with open(result_file, 'r') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        results[i] = line.split('\\n')[0].split('\\t')\n",
    "\n",
    "        \n",
    "        \n",
    "Exact_matches = []\n",
    "MaxFrag_matches = [] # Description in Supporting Information\n",
    "\n",
    "Exact_matches_multi = []\n",
    "MaxFrag_matches_mumlti = [] \n",
    "for i in range(len(results)):\n",
    "    match_exact = isomer_match(results[i], ground_truth[i], False)\n",
    "    match_maxfrag = isomer_match(results[i], ground_truth[i], True)\n",
    "    if len(rxn_ps[i].split('.')) > 1:\n",
    "        Exact_matches_multi.append(match_exact)\n",
    "        MaxFrag_matches_mumlti.append(match_maxfrag)\n",
    "    Exact_matches.append(match_exact)\n",
    "    MaxFrag_matches.append(match_maxfrag)\n",
    "    if i % 100 == 0:\n",
    "        print ('\\rCalculating accuracy... %s/%s' % (i, len(results)), end='', flush=True)\n",
    "        \n",
    "ks = [1, 3, 5, 10, 50]\n",
    "exact_k = {k:0 for k in ks}\n",
    "MaxFrag_k = {k:0 for k in ks}\n",
    "\n",
    "print(len(Exact_matches))\n",
    "for i in range(len(Exact_matches)):\n",
    "    for k in ks:\n",
    "        if Exact_matches[i] <= k and Exact_matches[i] != -1:\n",
    "            exact_k[k] += 1\n",
    "        if MaxFrag_matches[i] <= k and MaxFrag_matches[i] != -1:\n",
    "            MaxFrag_k[k] += 1\n",
    "\n",
    "with open('log.txt', 'a') as f:\n",
    "    f.write(f'\\n\\n{date.today()} using canonicalized test data: {canonicalize_data}; len testset: {len(results)}')\n",
    "    for k in ks:\n",
    "        print ('Top-%d Exact accuracy: %.3f, MaxFrag accuracy: %.3f' % \\\n",
    "               (k, exact_k[k]/len(Exact_matches), MaxFrag_k[k]/len(MaxFrag_matches)))\n",
    "        f.write(f'\\n\\tTop-{k} Exact accuracy: {exact_k[k]/len(Exact_matches):.3f}, MaxFrag accuracy: {MaxFrag_k[k]/len(MaxFrag_matches):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
